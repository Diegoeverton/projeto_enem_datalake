from pyspark.sql import SparkSession
import os

def criar_camada_bronze():
    # Inicializa a sessÃ£o do Spark configurada para otimizaÃ§Ã£o de Parquet
    spark = SparkSession.builder \
        .appName("IngestaoBronzeENEM") \
        .config("spark.sql.parquet.compression.codec", "snappy") \
        .getOrCreate()
    
    pasta_dados = '/app/dados'
    pasta_bronze = '/app/data_lake/bronze/enem'
    
    print("ğŸš€ Iniciando ingestÃ£o do Data Lake (Camada Bronze)...")
    
    # Itera sobre todas as pastas extraÃ­das (ex: microdados_enem_2020)
    for diretorio in os.listdir(pasta_dados):
        caminho_dir = os.path.join(pasta_dados, diretorio)
        
        # Ignora arquivos soltos tipo .zip
        if os.path.isdir(caminho_dir):
            ano = diretorio.split('_')[-1]
            nome_arquivo_csv = f"MICRODADOS_ENEM_{ano}.csv"
            caminho_csv = os.path.join(caminho_dir, 'DADOS', nome_arquivo_csv)
            
            # Se nÃ£o achar o arquivo minÃºsculo, testa maiÃºsculo genÃ©rico (por conta de case sensitive no linux)
            if not os.path.exists(caminho_csv):
                 caminho_csv = os.path.join(caminho_dir, 'DADOS', f"MICRODADOS_ENEM_{ano}.CSV")
            
            if os.path.exists(caminho_csv):
                print(f'â³ Processando ano {ano} a partir de: {caminho_csv}')
                
                # O formato do Inep/ENEM: CSV separado por ponto-e-vÃ­rgula e charset iso-8859-1 (latin1)
                df = spark.read.csv(
                    caminho_csv, 
                    header=True, 
                    sep=';', 
                    encoding='latin1'
                )
                
                # Destino: Particionando a pasta da camada bronze por ano
                pasta_saida = os.path.join(pasta_bronze, f"ano={ano}")
                
                print(f'ğŸ’¾ Convertendo para Parquet otimizado em: {pasta_saida}')
                
                # A conversÃ£o para parquet salva > 80% do espaÃ§o e aumenta a velocidade de leitura assustadoramente
                df.write.mode('overwrite').parquet(pasta_saida)
                
                print(f'âœ… Ano {ano} salvo no Data Lake com sucesso!\n')
            else:
                print(f'âš ï¸ Arquivo CSV nÃ£o encontrado na pasta {caminho_dir}/DADOS. Pulando...\n')

    print("ğŸ IngestÃ£o da Camada Bronze finalizada!")
    spark.stop()

if __name__ == "__main__":
    criar_camada_bronze()
