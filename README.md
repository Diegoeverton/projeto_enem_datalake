# ğŸ“Š Data Lake ENEM: Pipeline de Engenharia de Dados com Apache Spark

![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)
![Apache Spark](https://img.shields.io/badge/apache%20spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)
![Ubuntu](https://img.shields.io/badge/Ubuntu-E95420?style=for-the-badge&logo=ubuntu&logoColor=white)

Este projeto implementa uma arquitetura robusta de **Data Lake local** utilizando contÃªineres Docker para orquestrar a extraÃ§Ã£o, processamento e anÃ¡lise dos microdados do ENEM (Exame Nacional do Ensino MÃ©dio). 

O ambiente Ã© 100% isolado, idempotente e reproduzÃ­vel, trazendo o poder do Apache Spark (PySpark) para lidar com volumetria de Big Data (arquivos CSVs de mÃºltiplos gigabytes) convertendo-os em formatos colunares altamente otimizados.

---

## ğŸ—ï¸ Arquitetura do Projeto

A soluÃ§Ã£o foi projetada em duas frentes de serviÃ§o (rodando no Ubuntu via Docker):

1. **Pipeline de Engenharia (`spark`):** Uma esteira autÃ´noma que extrai os dados brutos e os converte em camadas estruturadas (Bronze e Prata) do Data Lake.
2. **LaboratÃ³rio AnalÃ­tico (`jupyter`):** Um ambiente interativo Jupyter Notebook para acesso aos dados refinados via `pandas` e `pyarrow`, sem consumir memÃ³ria excessiva.

### Camadas do Data Lake (Medallion Architecture)
* **Raw (Bruto):** Arquivos `.zip` originais na pasta `dados/`.
* **Bronze:** Dados ingeridos e convertidos de formato textual (CSV) para **Parquet** (compressÃ£o Snappy), particionados por ano.
* **Prata:** Tabela higienizada pelo PySpark, filtrando colunas analÃ­ticas e aplicando regras de negÃ³cio (como preenchimento de notas ausentes).

---

## ğŸš€ Como Executar o Projeto

### PrÃ©-requisitos
- [Docker](https://www.docker.com/products/docker-desktop) instalado e rodando.
- [Docker Compose](https://docs.docker.com/compose/install/) instalado.
- Arquivos `.zip` dos microdados do ENEM baixados do site do INEP e colocados na pasta raiz do projeto dentro de `dados/`.

### 1. Iniciar a Pipeline de Dados (Processamento do Data Lake)
O comando abaixo irÃ¡ providenciar toda a infraestrutura (Ubuntu, Java 17, PySpark) e executarÃ¡ o orquestrador `run_pipeline.sh`:
```bash
docker-compose up --build spark
```
Nesta etapa de execuÃ§Ã£o Ãºnica (batch), os arquivos gigantes deixarÃ£o de ser `.zip` e magicamente se materializarÃ£o no seu disco como tabelas limpas na pasta local `data_lake/prata/`.

### 2. Acessar o Ambiente de AnÃ¡lise (Jupyter)
Para iniciar as anÃ¡lises de CiÃªncia de Dados ou explorar os dados via Pandas interativamente, inicie o LaboratÃ³rio:
```bash
docker-compose up jupyter
```
No terminal logado do serviÃ§o, acesse a URL que contÃ©m o `token` (ex: `http://127.0.0.1:8888/?token=...`) diretamente no seu navegador. Os dados estarÃ£o no caminho do container `/app/data_lake/prata/enem`.

---

## ğŸ“ Estrutura de DiretÃ³rios e CÃ³digo
```text
ğŸ“¦ enem_datalake
 â”£ ğŸ“‚ dados/           # Armazena os .zips brutos baixados (Ignorado no Git para evitar limite de tamanho)
 â”£ ğŸ“‚ data_lake/       # RepositÃ³rio de dados processados em Parquet (Mapeado nos Volumes do Docker)
 â”ƒ â”£ ğŸ“‚ bronze/        
 â”ƒ â”— ğŸ“‚ prata/         
 â”£ ğŸ“‚ docs/            # DocumentaÃ§Ã£o rica e aprofundada voltada para o time e infraestrutura
 â”£ ğŸ“‚ notebooks/       # Armazena os relatÃ³rios .ipynb criados pelo time
 â”£ ğŸ“‚ src/             # Scripts primÃ¡rios da pipeline e de extraÃ§Ã£o
 â”ƒ â”£ ğŸ“œ extrair_dados_spark.py
 â”ƒ â”£ ğŸ“œ ingestao_bronze_spark.py
 â”ƒ â”£ ğŸ“œ processamento_prata_spark.py
 â”ƒ â”— ğŸ“œ run_pipeline.sh  # Orquestrador Mestre de bash
 â”£ ğŸ“œ .gitignore       # Blindagem do GitHub e do GitLocal (~100MB limit guard)
 â”£ ğŸ“œ docker-compose.yml 
 â”£ ğŸ“œ Dockerfile       # Receita do SO limpo, instÃ¢ncias Rootlsess e isolamento de usuÃ¡rios
 â”— ğŸ“œ requirements.txt # Bibliotecas padronizadas (Pandas, PyArrow, PySpark, JupyterLab)
```

## ğŸ“š DocumentaÃ§Ã£o Adicional / Manuais

Abaixo estÃ£o os guias detalhados encontrados na pasta `docs/` que acompanham o repositÃ³rio. Eles esclarecem as motivaÃ§Ãµes sobre negÃ³cio e infraestrutura deste Data Lake:
- **[Infraestrutura e Docker](docs/infraestrutura_docker.md):** Fundamentos do uso de Volumes e escolha da imagem.
- **[TransformaÃ§Ãµes - Camada Prata](docs/transformacoes_camada_prata.md):** Explicativo sobre a lÃ³gica de tratamento de dados (`NULLs`) e seleÃ§Ã£o vertical do INEP.
- **[Guia de ApresentaÃ§Ã£o ao Time](docs/guia_apresentacao_time.md):** Manual focado em como reproduzir a pipeline (idempotÃªncia) para os stakeholders.
- **[Oportunidade AnalÃ­tica & Insights](docs/insights_analiticos.md):** Matriz de estudos sociais, desigualdade geogrÃ¡fica e modelos recomendados de ML usando o Dataset Lapidado.

---
*Feito com â˜• e focado em orquestraÃ§Ã£o elegante por Diego.*
